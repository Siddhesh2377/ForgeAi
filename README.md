<p align="center">
  <img src="docs/images/forgeai-banner.svg" alt="ForgeAI Banner" width="100%" />
</p>

<h1 align="center">ğŸ”¨ ForgeAI</h1>

<p align="center">
  <strong>Your local AI model workshop â€” load, inspect, compress, train, merge, and test models entirely offline.</strong>
</p>

<p align="center">
  <img src="https://img.shields.io/badge/version-0.1.0-f59e0b?style=flat-square" alt="Version" />
  <img src="https://img.shields.io/badge/Tauri-v2-24C8D8?style=flat-square&logo=tauri&logoColor=white" alt="Tauri v2" />
  <img src="https://img.shields.io/badge/Svelte-5-FF3E00?style=flat-square&logo=svelte&logoColor=white" alt="Svelte 5" />
  <img src="https://img.shields.io/badge/Rust-2021-000000?style=flat-square&logo=rust&logoColor=white" alt="Rust" />
  <img src="https://img.shields.io/badge/license-MIT-green?style=flat-square" alt="MIT License" />
</p>

<p align="center">
  No cloud. No accounts. No telemetry. Everything runs on your hardware.
</p>

---

## ğŸ“‹ Table of Contents

- [Overview](#-overview)
- [Screenshots](#-screenshots)
- [Modules](#-modules)
  - [00 Dashboard](#00--dashboard)
  - [01 Load](#01--load)
  - [02 Inspect](#02--inspect)
  - [03 Compress](#03--compress)
  - [04 Hub](#04--hub)
  - [05 Convert](#05--convert)
  - [06 Training](#06--training)
  - [07 Settings](#07--settings)
  - [08 M-DNA Forge](#08--m-dna-forge)
  - [09 Test](#09--test)
  - [10 DataStudio](#10--datastudio)
- [Supported Formats](#-supported-formats)
- [Tech Stack](#-tech-stack)
- [Architecture](#-architecture)
- [Design System](#-design-system)
- [Getting Started](#-getting-started)
- [System Requirements](#-system-requirements)
- [Project Structure](#-project-structure)
- [License](#-license)

---

## ğŸ” Overview

ForgeAI is a **local-first desktop application** for working with AI models â€” from downloading and inspecting to fine-tuning, merging, and running inference. Built with **Tauri v2**, **SvelteKit 5**, and **Rust**, it provides a native, high-performance experience across Linux, macOS, and Windows.

### âœ¨ Key Highlights

| Feature | Description |
|---------|-------------|
| ğŸ”’ **Fully Offline** | No internet required after initial setup. Your models stay on your machine. |
| âš¡ **Native Performance** | Rust backend for tensor operations, model parsing, and merge execution |
| ğŸ§  **12 Merge Methods** | SLERP, TIES, DARE, DeLLa, Frankenmerge, MoE conversion, and more |
| ğŸ¯ **Smart Training** | Capability-targeted fine-tuning â€” train only the layers that matter |
| ğŸ”¬ **Deep Inspection** | 3D architecture visualization, SHA-256 fingerprinting, runtime compatibility |
| ğŸ“Š **DataStudio** | Load, analyze, and prepare datasets (JSON, JSONL, CSV, Parquet) with HuggingFace integration |
| ğŸ—ï¸ **Layer Surgery** | Remove or duplicate layers â€” pure Rust, no GPU required |
| ğŸ¨ **Industrial Design** | Technical spec sheet aesthetic with monospace fonts and amber accents |

---

## ğŸ“¸ Screenshots

<details>
<summary><strong>ğŸ–¥ï¸ Dashboard</strong> â€” System overview with module status tracking</summary>
<br/>
<img src="docs/images/light/dashboard-light.png" alt="Dashboard" width="100%" />
</details>

<details>
<summary><strong>ğŸ“‚ Load</strong> â€” Import GGUF, SafeTensors, or sharded model folders</summary>
<br/>
<img src="docs/images/light/load-light.png" alt="Load Module" width="100%" />
</details>

<details>
<summary><strong>ğŸ”¬ Inspect</strong> â€” 3D architecture, memory breakdown, quantization analysis</summary>
<br/>
<img src="docs/images/light/inspect-light.png" alt="Inspect Module" width="100%" />
<br/><br/>
<img src="docs/images/light/inspect-caps-light.png" alt="Inspect Capabilities" width="100%" />
</details>

<details>
<summary><strong>ğŸ“¦ Compress</strong> â€” Quantize GGUF models (Q2_K â†’ F16)</summary>
<br/>
<img src="docs/images/light/compress-light.png" alt="Compress Module" width="100%" />
</details>

<details>
<summary><strong>ğŸŒ Hub</strong> â€” Search HuggingFace & manage local library</summary>
<br/>
<img src="docs/images/light/hub-light.png" alt="Hub Search" width="100%" />
<br/><br/>
<img src="docs/images/light/hub-lib-light.png" alt="Hub Library" width="100%" />
</details>

<details>
<summary><strong>ğŸ”„ Convert</strong> â€” SafeTensors â†’ GGUF conversion</summary>
<br/>
<img src="docs/images/light/convert.png" alt="Convert Module" width="100%" />
</details>

<details>
<summary><strong>ğŸ¯ Training</strong> â€” Fine-tuning & layer surgery</summary>
<br/>
<img src="docs/images/light/traning-finetuning-light.png" alt="Training Fine-tune" width="100%" />
<br/><br/>
<img src="docs/images/light/traning-surgery.png" alt="Training Surgery" width="100%" />
</details>

<details>
<summary><strong>ğŸ§¬ M-DNA Forge</strong> â€” Merge models with 12 methods</summary>
<br/>
<img src="docs/images/light/m-dna-setup.png" alt="M-DNA Setup" width="100%" />
<br/><br/>
<img src="docs/images/light/m-dna-presets.png" alt="M-DNA Presets" width="100%" />
</details>

<details>
<summary><strong>â–¶ï¸ Test</strong> â€” Run inference with real-time token streaming</summary>
<br/>
<img src="docs/images/light/test.png" alt="Test Module" width="100%" />
</details>

<details>
<summary><strong>ğŸ“Š DataStudio</strong> â€” Explore & prepare datasets</summary>
<br/>
<img src="docs/images/light/data-studio-light.png" alt="DataStudio" width="100%" />
</details>

<details>
<summary><strong>âš™ï¸ Settings</strong> â€” Environment management & configuration</summary>
<br/>
<img src="docs/images/light/settings.png" alt="Settings" width="100%" />
</details>

---

## ğŸ§© Modules

ForgeAI is organized into **11 modules** grouped into three categories:

| Category | Modules |
|----------|---------|
| **MODEL** | Load, Inspect, Compress |
| **DATA** | Hub, DataStudio, Training |
| **TOOLS** | Convert, M-DNA Forge, Test |
| **SYSTEM** | Dashboard, Settings |

---

### 00 Â· Dashboard

> **System command center** â€” real-time overview of all modules and active tasks.

The dashboard provides a bird's-eye view of your entire workflow:

- **System Status Banner** â€” shows current state (IDLE / LOADING / TRAINING / MERGING / COMPLETE)
- **Loaded Model Specs** â€” file name, format, parameters, size, quantization level
- **Module Cards** â€” all 11 modules organized in MODEL / DATA / TOOLS groups
- **Live Activity Badges** â€” real-time progress on each module (e.g., "TRAINING 45%", "MERGING 72%")
- **Quick Navigation** â€” click any module card to jump directly to it

Each module card shows:
- Module code and name
- Short description of its function
- Supported formats/operations
- Current status (ready / awaiting model / active task)

---

### 01 Â· Load

> **Model import** â€” load GGUF files, SafeTensors files, or sharded HuggingFace model directories.

| Input Type | How |
|-----------|-----|
| **GGUF file** | Browse for a single `.gguf` file |
| **SafeTensors file** | Browse for a single `.safetensors` file |
| **SafeTensors directory** | Select a folder containing sharded SafeTensors + config files |

Once loaded, the model is available globally across all modules â€” Inspect, Compress, Training, Test, and more. The status bar at the bottom shows the loaded model's name, format, and parameter count at all times.

**Displayed Info:**
- File name and full path
- File size
- Format (GGUF / SafeTensors)
- Architecture (e.g., LlamaForCausalLM)
- Parameter count
- Quantization type (for GGUF)
- Shard count (for multi-file models)

---

### 02 Â· Inspect

> **Deep model analysis** â€” architecture visualization, memory layout, capability detection, runtime compatibility.

Inspect provides a comprehensive X-ray of any loaded model.

#### ğŸ—ï¸ 3D Isometric Architecture Visualization
An interactive isometric tower view of the model's layer structure. Hover over layers to see details. Visual representation of attention heads, MLP blocks, and normalization components.

#### ğŸ“Š Memory Distribution
Six-component breakdown showing how memory is allocated:

| Component | What |
|-----------|------|
| Embeddings | Token embedding weights |
| Attention | Q/K/V/O projection matrices |
| MLP | Gate, up, and down projections |
| Norms | RMSNorm / LayerNorm weights |
| Output | Language model head |
| Other | Miscellaneous tensors |

Each component shows exact byte count and percentage with visual bars.

#### ğŸ”¢ Quantization Breakdown
Per-dtype analysis of all tensors â€” shows distribution across F32, F16, BF16, Q8_0, Q4_K_M, etc. with visual bar chart.

#### ğŸ–¥ï¸ Runtime Compatibility Matrix
Checks support across 8 popular inference runtimes:

| Runtime | Checks |
|---------|--------|
| llama.cpp | Format, quantization compatibility |
| Ollama | Format support |
| LM Studio | Format and architecture support |
| GPT4All | Format support |
| Kobold.cpp | GGUF compatibility |
| Jan | Format support |
| LocalAI | Format support |
| text-generation-webui | Format and architecture |

#### ğŸ§  Capability Detection
Analyzes model architecture to detect 7 capabilities with confidence scores:

| Capability | What It Detects |
|-----------|----------------|
| ğŸ”§ Tool Calling | API/function calling ability |
| ğŸ§  Reasoning | Chain-of-thought reasoning |
| ğŸ’» Code | Code generation/understanding |
| ğŸ”¢ Mathematics | Mathematical reasoning |
| ğŸŒ Multilingual | Multi-language support |
| ğŸ“‹ Instruction | Instruction following |
| ğŸ›¡ï¸ Safety | Safety/alignment layers |

#### ğŸ” SHA-256 Fingerprint
Compute and verify file integrity with cryptographic hashing.

#### ğŸ“‹ Additional Panels
- **Configuration** â€” all model hyperparameters (hidden size, head count, layers, vocab size, etc.)
- **Attention Architecture** â€” GQA visualization with query/key-value head ratios
- **Tokenizer Info** â€” special tokens (BOS, EOS, PAD, UNK) and vocabulary size
- **Layer Hierarchy** â€” expandable block structure with nested components
- **Tensor Browser** â€” searchable, filterable list of all tensors with name, shape, dtype, and size
- **Export** â€” JSON or CSV export of the full inspection report

---

### 03 Â· Compress

> **GGUF quantization** â€” reduce model size while preserving quality.

Quantize GGUF models across 7 levels with real-time size and quality estimation.

#### Quantization Levels

| Level | Bits/Weight | Quality | Speed | Use Case |
|-------|:-----------:|:-------:|:-----:|----------|
| **Q2_K** | 2.6 | â­ | âš¡âš¡âš¡âš¡âš¡ | Maximum compression, minimal quality |
| **Q3_K_M** | 3.4 | â­â­ | âš¡âš¡âš¡âš¡ | Mobile / Edge devices |
| **Q4_K_M** | 4.5 | â­â­â­ | âš¡âš¡âš¡ | Best balance of size and quality |
| **Q5_K_M** | 5.5 | â­â­â­â­ | âš¡âš¡ | High quality with good compression |
| **Q6_K** | 6.5 | â­â­â­â­ | âš¡âš¡ | Near-original quality |
| **Q8_0** | 8.0 | â­â­â­â­â­ | âš¡ | Minimal quality loss |
| **F16** | 16.0 | â­â­â­â­â­ | â€” | Full precision (half-float) |

#### Quick Presets

| Preset | Level | Target |
|--------|-------|--------|
| ğŸ“± **MOBILE** | Q3_K_M | Small devices, maximum compression |
| âš–ï¸ **BALANCED** | Q4_K_M | Best all-around choice |
| ğŸ¯ **QUALITY** | Q6_K | Quality-first with good compression |

#### Features
- **Before/After Comparison** â€” estimated file size and memory reduction
- **Component Breakdown** â€” memory per component type at the target quantization
- **Requantization Warning** â€” alerts when quantizing an already-quantized model (quality loss)
- **Progress Tracking** â€” real-time quantization progress with ETA
- Uses **llama.cpp** under the hood (auto-downloaded via Settings)

---

### 04 Â· Hub

> **Model discovery & management** â€” search HuggingFace, download models, manage your local library.

#### ğŸ” Search Mode
- Enter any HuggingFace repository ID (e.g., `TheBloke/Llama-2-7B-GGUF`)
- View all files in the repository with size, format, and download buttons
- Download individual files or entire repositories
- Real-time download progress with speed and percentage
- Cancel downloads at any time

#### ğŸ“š Library Mode
- Browse all locally downloaded models
- View metadata: file name, size, format, source repository, download date
- Total storage tracking across all models
- Delete models to free space
- Import existing local models/folders into the library

---

### 05 Â· Convert

> **Format conversion** â€” transform SafeTensors models into GGUF format for efficient inference.

#### Workflow
1. **Select Source** â€” pick a SafeTensors model directory (or select from Hub downloads)
2. **Auto-Detect** â€” ForgeAI analyzes the model architecture, layer count, vocab size, hidden dimensions
3. **Choose Output Type** â€” select conversion precision
4. **Convert** â€” watch progress stage-by-stage

#### Output Types

| Type | Description | Use Case |
|------|-------------|----------|
| **F16** | Half-precision float | Good balance of size and precision |
| **BF16** | Brain floating point | Better for models trained in BF16 |
| **F32** | Full precision | Maximum accuracy, largest size |
| **Q8_0** | 8-bit quantized | Smallest output, ready for inference |
| **AUTO** | Automatic | Uses the model's native precision |

#### Auto-Detection
The converter automatically identifies:
- `config.json` â€” model architecture and hyperparameters
- `tokenizer.json` / `tokenizer.model` â€” tokenizer files
- `tokenizer_config.json` â€” tokenizer configuration
- `*.safetensors` â€” all sharded weight files

Uses a **separate Python environment** (managed via Settings) with the HuggingFace conversion scripts.

---

### 06 Â· Training

> **Fine-tuning & layer surgery** â€” adapt models with GPU-accelerated training or perform pure-Rust tensor operations.

Training has two modes: **Fine-Tune** and **Layer Surgery**.

#### ğŸ¯ Fine-Tune Mode

##### Training Methods

| Method | Description | VRAM Needed |
|--------|-------------|:-----------:|
| **LoRA** | Low-Rank Adaptation â€” efficient adapter training | 6â€“12 GB |
| **QLoRA** | Quantized LoRA â€” 4-bit base model with LoRA adapters | 4â€“8 GB |
| **SFT** | Supervised Fine-Tuning â€” standard training on datasets | 8â€“24 GB |
| **DPO** | Direct Preference Optimization â€” learn from chosen/rejected pairs | 8â€“24 GB |
| **Full** | Full parameter update â€” maximum quality, highest requirements | 16â€“48 GB |

##### Training Presets

| Preset | VRAM | Method | Rank | Seq Length |
|--------|:----:|--------|:----:|:----------:|
| ğŸ”‹ **LOW VRAM** | ~4 GB | QLoRA | 8 | 256 |
| âš–ï¸ **BALANCED** | ~6 GB | QLoRA | 16 | 512 |
| ğŸ¯ **QUALITY** | ~12 GB | LoRA | 32 | 1024 |
| ğŸ† **MAX QUALITY** | ~24 GB | LoRA | 64 | 2048 |

##### Hyperparameters
Full control over all training parameters:
- Learning rate, epochs, batch size
- Gradient accumulation steps
- Max sequence length
- Warmup steps, weight decay
- Save steps interval
- LoRA rank, alpha, dropout
- Quantization bits (4/8 for QLoRA)
- DPO beta

##### ğŸ§  Capability-Targeted Layer Selection
Instead of fine-tuning the entire model, target specific capabilities:

| Capability | Affected Layers | What It Trains |
|-----------|:--------------:|---------------|
| ğŸ”§ Tool Calling | Upper-mid | Teach the model to use tools/APIs |
| ğŸ§  Reasoning / CoT | Mid-upper | Improve chain-of-thought reasoning |
| ğŸ’» Code Generation | Upper-mid | Enhance code writing ability |
| ğŸ”¢ Mathematics | Mid | Improve mathematical reasoning |
| ğŸŒ Multilingual | Early-mid | Add or improve language support |
| ğŸ“‹ Instruction Following | Mid | Better adherence to instructions |
| ğŸ›¡ï¸ Safety & Alignment | Final | Adjust safety/alignment behavior |

##### Target Module Detection
Auto-detects available LoRA target modules from the model architecture:
`q_proj` Â· `k_proj` Â· `v_proj` Â· `o_proj` Â· `gate_proj` Â· `up_proj` Â· `down_proj`

##### Dataset Support
- **Auto-detection** of dataset templates: Alpaca, ShareGPT, ChatML, DPO pairs, Text, Prompt/Completion
- **Supported formats**: JSON, JSONL, CSV, Parquet
- **Preview**: View dataset rows and column structure before training

##### Live Training Dashboard
- Real-time epoch/step/loss/learning rate monitoring
- Step-by-step loss history chart
- GPU memory (VRAM) usage tracking
- ETA and time remaining
- Option to merge adapter back into base model after training

#### ğŸ”ª Layer Surgery Mode

Pure Rust tensor operations â€” **no Python or GPU required**.

| Operation | Description |
|-----------|-------------|
| **Remove Layers** | Select and strip unnecessary layers to reduce model size |
| **Duplicate Layers** | Clone layers at specific positions to increase depth |

##### Features
- **Rich Layer Table** â€” memory breakdown per layer with component bars (attention / MLP / norm %)
- **Tensor-Level Inspection** â€” expand any layer to see every tensor's dtype, shape, and memory
- **Surgery Preview** â€” shows final layer count before execution
- **Format Support** â€” works with both SafeTensors directories and GGUF files
- **Auto-Update** â€” automatically updates `config.json` / GGUF metadata with new layer counts

---

### 07 Â· Settings

> **Application configuration** â€” theme, fonts, GPU detection, and environment management.

#### ğŸ¨ Appearance
- **Theme Toggle** â€” Dark mode (default) / Light mode
- **Font Family** â€” choose from system monospace fonts (JetBrains Mono preferred)
- **Font Size** â€” adjustable base font size

#### ğŸ–¥ï¸ GPU Detection
- Automatic CUDA/GPU detection
- Displays GPU name, VRAM, driver version

#### ğŸ”§ Environment Management
Manage all three tool environments from one place:

| Environment | What It Manages |
|-------------|----------------|
| **llama.cpp** | GGUF inference & quantization binary â€” download, update, or remove |
| **Training** | Python venv with PyTorch, Transformers, PEFT, TRL, BitsAndBytes â€” setup, view packages, clean |
| **Convert** | Python venv with HuggingFace conversion scripts â€” setup, view packages, clean |

Each environment shows:
- Installation status (installed / partial / not installed)
- Python version and venv path
- Installed packages list
- CUDA availability
- **Clean/Delete** button to remove the environment and start fresh

---

### 08 Â· M-DNA Forge

> **Model merging** â€” combine 2â€“5 parent models into hybrid offspring with full control over strategy and layer composition.

#### ğŸ§¬ 12 Merge Methods

| Method | Difficulty | Description |
|--------|:----------:|-------------|
| **Average** | Easy | Simple weighted average of tensors |
| **SLERP** | Easy | Spherical linear interpolation â€” best for 2-model merges |
| **Passthrough** | Easy | Direct copy from a single parent |
| **Task Arithmetic** | Intermediate | Add task vectors from multiple finetunes to a base model |
| **Frankenmerge** | Intermediate | Cherry-pick specific layers from specific parents |
| **DARE** | Intermediate | Drop and rescale â€” prunes delta parameters with random dropout |
| **TIES** | Intermediate | Trim, elect sign, merge â€” resolves task vector interference |
| **DeLLa** | Advanced | Density-based layer-level adaptive merging with lambda |
| **Component Merge** | Advanced | Route attention/MLP/norm components to different parents |
| **Tensor Surgery** | Advanced | Per-tensor source mapping from any parent |
| **Parameter Slice** | Advanced | Dimensional slicing across parent tensors |
| **MoE Conversion** | Advanced | Convert dense models into Mixture-of-Experts architecture |

#### âš¡ Quick Presets

| Preset | Method | Key Params | Best For |
|--------|--------|-----------|----------|
| ğŸ¯ **Quick Blend** | Average | â€” | Simple, fast merging |
| ğŸŒ€ **Smooth Merge** | SLERP | t = 0.5 | Balanced interpolation |
| ğŸ”§ **Task Tuner** | Task Arithmetic | scaling = 1.0 | Adding capabilities |
| ğŸ² **Sparse Mix** | DARE | density = 0.5 | Efficient delta merging |
| ğŸ—³ï¸ **Consensus** | TIES | trim = 0.2 | Conflict resolution |

#### ğŸ—ï¸ 3D Isometric Visualization
Interactive tower view showing all parent models and the resulting offspring. Each layer is color-coded by its source parent. Pan, zoom, and hover for details.

#### ğŸ›ï¸ Three Difficulty Modes

| Mode | Control Level | Best For |
|------|:------------:|---------|
| **Easy** | Basic settings only | First-time users |
| **Intermediate** | Method params + layer assignment | Most merges |
| **Advanced** | Full tensor/component control | Expert users |

#### Features
- **Layer Assignment** â€” manually assign each layer to a parent, or use auto-assign (Split / Interleave)
- **Capability Detection** â€” detect and filter layers by capability (reasoning, code, math, etc.)
- **Layer Analysis** â€” specialization profiling (Syntactic / Semantic / Reasoning)
- **Compatibility Check** â€” validates architecture and dimension compatibility across parents
- **Batch Size Control** â€” 1â€“16 tensors processed concurrently (higher = faster, more RAM)
- **Composition Stats** â€” real-time parent weight and layer distribution
- **Output Formats** â€” SafeTensors (HF-compatible directory) or GGUF (with embedded tokenizer)
- **Background Execution** â€” navigate freely while the merge runs
- **Progress Tracking** â€” real-time progress in the sidebar and status bar

---

### 09 Â· Test

> **Model inference** â€” run prompts through your models with real-time token streaming.

Supports both **GGUF** (via llama.cpp) and **SafeTensors** (via HuggingFace Transformers) models.

#### ğŸ§ª Quick Test Presets

| Preset | Prompt Theme | Tests |
|--------|-------------|-------|
| ğŸ’» **CODE** | FizzBuzz in Python | Code generation ability |
| ğŸ”¢ **MATH** | Word problem solving | Mathematical reasoning |
| ğŸ§  **REASON** | Logic puzzle | Logical deduction |
| ğŸ¨ **CREATIVE** | Story writing | Creative writing |
| ğŸ“‹ **INSTRUCT** | Step-by-step tasks | Instruction following |
| ğŸ’¬ **CHAT** | Conversational | General chat ability |

#### Generation Settings

| Parameter | Range | Default | Description |
|-----------|:-----:|:-------:|-------------|
| Max Tokens | 1â€“8192 | 256 | Maximum output length |
| Temperature | 0.0â€“2.0 | 0.7 | Randomness (0 = deterministic) |
| Top-p | 0.0â€“1.0 | 0.9 | Nucleus sampling threshold |
| Top-k | 1â€“100 | 40 | Top-k token sampling |
| Repeat Penalty | 1.0â€“2.0 | 1.1 | Repetition suppression |
| Context Size | 512â€“32768 | 2048 | Context window size |
| GPU Layers | -1 to 99 | -1 | Layer offloading (-1=auto, 0=CPU) |

#### Features
- **System Prompt** â€” custom system message for the model
- **Local Library Integration** â€” select models from your Hub downloads
- **Real-Time Streaming** â€” tokens appear as they're generated
- **Performance Metrics** â€” tokens/sec, total generation time, device used (CPU/GPU)
- **Cancel** â€” stop generation mid-stream at any time

---

### 10 Â· DataStudio

> **Dataset explorer** â€” load, analyze, and prepare datasets from local files or HuggingFace.

#### ğŸ“ Local Mode
- Browse and load dataset files from disk
- Supports **JSON**, **JSONL**, **CSV**, and **Parquet** formats

#### ğŸŒ HuggingFace Mode
- Search datasets by repository ID (e.g., `tatsu-lab/alpaca`)
- View available files with format badges and file sizes
- Download individual files with real-time progress tracking
- Auto-loads the dataset after download completes

#### ğŸ“Š Analysis Features

| Feature | Description |
|---------|-------------|
| **Metadata** | File path, format, row count, file size, column count |
| **Template Detection** | Auto-detects dataset template (Alpaca, ShareGPT, ChatML, DPO, Text, etc.) |
| **Column Analysis** | Per-column dtype, valid count, null count, average string length |
| **Null Detection** | Highlights columns with null values for data quality checks |
| **Data Preview** | Scrollable table showing first N rows with cell truncation |
| **Parquet Support** | Native Rust Parquet reader using Apache Arrow |

---

## ğŸ“ Supported Formats

### Model Formats

| Format | Load | Inspect | Compress | Convert | Merge | Train | Surgery | Test |
|--------|:----:|:-------:|:--------:|:-------:|:-----:|:-----:|:-------:|:----:|
| **GGUF** | âœ… | âœ… | âœ… | output | âœ… | âœ… | âœ… | âœ… |
| **SafeTensors** | âœ… | âœ… | â€” | input | âœ… | âœ… | âœ… | âœ… |
| **Sharded Folders** | âœ… | âœ… | â€” | input | âœ… | âœ… | âœ… | âœ… |

### Dataset Formats

| Format | DataStudio | Training | HuggingFace |
|--------|:----------:|:--------:|:-----------:|
| **JSON** | âœ… | âœ… | âœ… |
| **JSONL** | âœ… | âœ… | âœ… |
| **CSV** | âœ… | âœ… | âœ… |
| **Parquet** | âœ… | âœ… | âœ… |

---

## âš™ï¸ Tech Stack

| Layer | Technology | Role |
|-------|-----------|------|
| ğŸ–¥ï¸ **Shell** | [Tauri v2](https://v2.tauri.app/) | Native desktop window (Linux, macOS, Windows) |
| ğŸ¨ **Frontend** | [SvelteKit 5](https://svelte.dev/) (Svelte 5 runes) | Reactive UI with `$state`, `$derived`, `$effect`, `$props` |
| âš™ï¸ **Backend** | [Rust](https://www.rust-lang.org/) (2021 edition) | Model parsing, tensor operations, merge execution, layer surgery |
| ğŸ§® **Tensors** | [Candle](https://github.com/huggingface/candle) | Rust ML framework for tensor math (SLERP, TIES, DARE, etc.) |
| ğŸ“¦ **GGUF** | [llama.cpp](https://github.com/ggerganov/llama.cpp) | Quantization and GGUF inference with GPU support |
| ğŸ¯ **Training** | [PyTorch](https://pytorch.org/) + [PEFT](https://github.com/huggingface/peft) + [TRL](https://github.com/huggingface/trl) | GPU fine-tuning via managed Python subprocess |
| ğŸ”¬ **Inference** | [HuggingFace Transformers](https://github.com/huggingface/transformers) | SafeTensors inference via Python |
| ğŸŒ **Hub** | [HuggingFace API](https://huggingface.co/) | Model and dataset discovery/download |
| ğŸ“Š **Parquet** | [Apache Arrow](https://arrow.apache.org/) + [Parquet](https://parquet.apache.org/) | Native Rust dataset parsing |
| âš¡ **Async** | [Tokio](https://tokio.rs/) | Non-blocking task execution |
| ğŸ”„ **Serialization** | [Serde](https://serde.rs/) | Rust â†” Frontend data exchange |
| ğŸ“ **Dialogs** | [tauri-plugin-dialog](https://github.com/nicovank/tauri-plugin-dialog) | Native file picker dialogs |

---

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     SvelteKit 5 Frontend                    â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚Dashboard â”‚ â”‚  Load    â”‚ â”‚ Inspect  â”‚ â”‚ Compress â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Hub    â”‚ â”‚DataStudioâ”‚ â”‚ Training â”‚ â”‚ Convert  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚  M-DNA   â”‚ â”‚   Test   â”‚ â”‚ Settings â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                                             â”‚
â”‚  Svelte 5 Stores: model Â· dna Â· training Â· hub Â· test Â·    â”‚
â”‚                   convert Â· datastudio Â· theme              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚  Tauri IPC (invoke / events)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Rust Backend                           â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Model Parser  â”‚  â”‚ Merge Engine (M-DNA)            â”‚     â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚     â”‚
â”‚  â”‚ GGUF headers  â”‚  â”‚ 12 methods (SLERP, TIES, DARE, â”‚     â”‚
â”‚  â”‚ SafeTensors   â”‚  â”‚ DeLLa, Frankenmerge, MoE, ...)  â”‚     â”‚
â”‚  â”‚ Multi-shard   â”‚  â”‚ Candle tensor operations        â”‚     â”‚
â”‚  â”‚ Tensor index  â”‚  â”‚ Layer profiler & analyzer       â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Quantizer     â”‚  â”‚ Training Engine                 â”‚     â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚     â”‚
â”‚  â”‚ llama.cpp     â”‚  â”‚ Python subprocess (venv)        â”‚     â”‚
â”‚  â”‚ 7 quant types â”‚  â”‚ PyTorch + PEFT + TRL            â”‚     â”‚
â”‚  â”‚ GPU support   â”‚  â”‚ CUDA auto-detection             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ HF Hub Client â”‚  â”‚ Layer Surgery                   â”‚     â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚     â”‚
â”‚  â”‚ Model search  â”‚  â”‚ Pure Rust tensor remapping      â”‚     â”‚
â”‚  â”‚ Dataset fetch â”‚  â”‚ Remove / duplicate layers       â”‚     â”‚
â”‚  â”‚ File download â”‚  â”‚ GGUF & SafeTensors support      â”‚     â”‚
â”‚  â”‚ Progress emit â”‚  â”‚ Auto metadata update            â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Converter     â”‚  â”‚ Parquet / Arrow Reader          â”‚     â”‚
â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚     â”‚
â”‚  â”‚ ST â†’ GGUF     â”‚  â”‚ Native Rust parsing             â”‚     â”‚
â”‚  â”‚ Python venv   â”‚  â”‚ Schema extraction               â”‚     â”‚
â”‚  â”‚ 5 output typesâ”‚  â”‚ JSON serialization              â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Event System

Real-time communication between backend and frontend via Tauri events:

| Event | Source | Description |
|-------|--------|-------------|
| `training:progress` | Training Engine | Epoch, step, loss, learning rate, ETA |
| `training:setup-progress` | Venv Setup | Package installation progress |
| `training:surgery-progress` | Layer Surgery | Layer processing progress |
| `convert:progress` | Converter | Conversion stage progress |
| `convert:setup-progress` | Venv Setup | Package installation progress |
| `hub:download-progress` | HF Client | File download bytes/percent |
| `datastudio:download-progress` | HF Client | Dataset download bytes/percent |
| `test:token` | Inference Engine | Individual streamed tokens |
| `merge:progress` | Merge Engine | Tensor processing progress |
| `merge:profile-progress` | Profiler | Layer profiling progress |

---

## ğŸ¨ Design System

ForgeAI uses an **industrial label / technical spec sheet** aesthetic â€” inspired by product labels, engineering documentation, and industrial control panels.

### Principles

| Principle | Implementation |
|-----------|---------------|
| **Monospace** | JetBrains Mono (system monospace stack) everywhere |
| **Sharp** | 0px border-radius â€” no rounded corners anywhere |
| **Structured** | 1px borders, grid layouts, consistent 8px spacing |
| **Labeled** | ALL CAPS with letter-spacing for labels and headers |
| **Decorated** | Corner marks on panels, barcode patterns, serial identifiers |

### Color Semantics

| Color | Token | Hex | Meaning |
|-------|-------|-----|---------|
| ğŸŸ  Amber | `--accent` | `#f59e0b` | Brand / Primary / Idle |
| ğŸ”µ Blue | `--info` | `#3b82f6` | Working / In Progress |
| ğŸŸ¢ Green | `--success` | `#22c55e` | Success / Complete |
| ğŸ”´ Red | `--danger` | `#ef4444` | Error / Failure |
| âšª Gray | `--text-muted` | `#525252` | Inactive / Disabled |

### Themes

| Theme | Background | Surface | Text | Borders |
|-------|-----------|---------|------|---------|
| ğŸŒ‘ **Dark** (default) | `#0a0a0a` | `#121212` | `#ffffff` | `#262626` |
| ğŸŒ• **Light** | `#fafafa` | `#ffffff` | `#0a0a0a` | `#e5e5e5` |

---

## ğŸš€ Getting Started

### Prerequisites

| Requirement | Version | Required |
|------------|---------|:--------:|
| [Rust](https://rustup.rs/) | Latest stable | âœ… |
| [Node.js](https://nodejs.org/) | v20+ | âœ… |
| [Tauri v2 Prerequisites](https://v2.tauri.app/start/prerequisites/) | Per your OS | âœ… |
| [Python](https://python.org/) | 3.10+ | Optional (for Training & Convert) |
| NVIDIA CUDA | 11.8+ | Optional (for GPU training) |

### Install & Run

```bash
# Clone the repository
git clone https://github.com/your-username/forgeai.git
cd forgeai

# Install frontend dependencies
npm install

# Run in development mode
npm run tauri dev
```

### Build for Production

```bash
npm run tauri build
```

The compiled binary will be in `src-tauri/target/release/`.

### Quick Start Guide

1. **Launch ForgeAI** â€” the dashboard shows all available modules
2. **Load a Model** â†’ Go to `01 LOAD` and import a GGUF or SafeTensors file
3. **Inspect It** â†’ Go to `02 INSPECT` for architecture visualization, memory breakdown, and capabilities
4. **Compress It** â†’ Go to `03 COMPRESS` to quantize to a smaller size
5. **Download More** â†’ Go to `04 HUB` to search and download models from HuggingFace
6. **Prepare Data** â†’ Go to `10 DATASTUDIO` to load and explore training datasets
7. **Fine-Tune** â†’ Go to `06 TRAINING` with a model + dataset to start training
8. **Merge Models** â†’ Go to `08 M-DNA` to combine multiple models into one
9. **Test It** â†’ Go to `09 TEST` to run inference and see the output

---

## ğŸ’» System Requirements

| | Minimum | Recommended |
|:-:|---------|-------------|
| **OS** | Linux, macOS, Windows | â€” |
| **RAM** | 8 GB | 16 GB+ |
| **Disk** | 2 GB + model storage | SSD with 50 GB+ free |
| **GPU** | Not required | NVIDIA (CUDA) / AMD (Vulkan) / Apple Silicon (Metal) |
| **Training GPU** | NVIDIA, 4 GB+ VRAM (QLoRA) | NVIDIA, 8 GB+ VRAM |
| **Merge** | CPU-only supported | 16 GB+ RAM for large models |

### GPU Support Matrix

| Platform | Inference (llama.cpp) | Training (PyTorch) | Merge (Candle) |
|----------|:--------------------:|:-----------------:|:--------------:|
| NVIDIA CUDA | âœ… | âœ… | âœ… |
| AMD Vulkan | âœ… | âŒ | âŒ |
| Apple Metal | âœ… | âœ… (MPS) | âŒ |
| CPU Only | âœ… | âœ… (slow) | âœ… |

---

## ğŸ“ Project Structure

```
forgeai/
â”œâ”€â”€ src/                              # SvelteKit 5 Frontend
â”‚   â”œâ”€â”€ routes/                       # Page routes
â”‚   â”‚   â”œâ”€â”€ +layout.svelte            #   App shell (header, sidebar, statusbar)
â”‚   â”‚   â”œâ”€â”€ +layout.ts                #   SSR disabled
â”‚   â”‚   â”œâ”€â”€ +page.svelte              #   00 Dashboard
â”‚   â”‚   â”œâ”€â”€ load/+page.svelte         #   01 Load
â”‚   â”‚   â”œâ”€â”€ inspect/+page.svelte      #   02 Inspect
â”‚   â”‚   â”œâ”€â”€ optimize/+page.svelte     #   03 Compress
â”‚   â”‚   â”œâ”€â”€ hub/+page.svelte          #   04 Hub
â”‚   â”‚   â”œâ”€â”€ convert/+page.svelte      #   05 Convert
â”‚   â”‚   â”œâ”€â”€ training/+page.svelte     #   06 Training
â”‚   â”‚   â”œâ”€â”€ settings/+page.svelte     #   07 Settings
â”‚   â”‚   â”œâ”€â”€ dna/+page.svelte          #   08 M-DNA Forge
â”‚   â”‚   â”œâ”€â”€ test/+page.svelte         #   09 Test
â”‚   â”‚   â””â”€â”€ datastudio/+page.svelte   #   10 DataStudio
â”‚   â”‚
â”‚   â”œâ”€â”€ lib/                          # Svelte 5 stores & utilities
â”‚   â”‚   â”œâ”€â”€ model.svelte.ts           #   Model state (load/unload/info)
â”‚   â”‚   â”œâ”€â”€ dna.svelte.ts             #   M-DNA merge state
â”‚   â”‚   â”œâ”€â”€ training.svelte.ts        #   Training & surgery state
â”‚   â”‚   â”œâ”€â”€ hub.svelte.ts             #   HuggingFace hub state
â”‚   â”‚   â”œâ”€â”€ test.svelte.ts            #   Inference state
â”‚   â”‚   â”œâ”€â”€ convert.svelte.ts         #   Conversion state
â”‚   â”‚   â”œâ”€â”€ datastudio.svelte.ts      #   DataStudio state
â”‚   â”‚   â””â”€â”€ theme.svelte.ts           #   Theme & font state
â”‚   â”‚
â”‚   â””â”€â”€ app.css                       # Global design system
â”‚
â”œâ”€â”€ src-tauri/                        # Rust Backend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ lib.rs                    #   Tauri app setup + 50+ command registrations
â”‚   â”‚   â”œâ”€â”€ commands.rs               #   Core commands (load, inspect, compress, hub, convert, test, settings)
â”‚   â”‚   â”œâ”€â”€ merge_commands.rs         #   M-DNA merge commands (18 commands)
â”‚   â”‚   â”œâ”€â”€ training_commands.rs      #   Training commands (13 commands)
â”‚   â”‚   â”œâ”€â”€ model/                    #   Model parsing
â”‚   â”‚   â”‚   â”œâ”€â”€ state.rs              #     AppState (shared Tauri state)
â”‚   â”‚   â”‚   â””â”€â”€ error.rs              #     Error types
â”‚   â”‚   â”œâ”€â”€ merge/                    #   Merge engine
â”‚   â”‚   â”‚   â”œâ”€â”€ config.rs             #     MergeConfig + method definitions
â”‚   â”‚   â”‚   â”œâ”€â”€ executor.rs           #     Merge execution engine
â”‚   â”‚   â”‚   â”œâ”€â”€ planner.rs            #     Layer assignment planning
â”‚   â”‚   â”‚   â”œâ”€â”€ registry.rs           #     Method registry (12 methods)
â”‚   â”‚   â”‚   â”œâ”€â”€ tensor_io.rs          #     Tensor read/write (GGUF + SafeTensors)
â”‚   â”‚   â”‚   â”œâ”€â”€ output.rs             #     Output format handling
â”‚   â”‚   â”‚   â”œâ”€â”€ compatibility.rs      #     Architecture compatibility checking
â”‚   â”‚   â”‚   â”œâ”€â”€ capabilities.rs       #     Layer capability detection
â”‚   â”‚   â”‚   â”œâ”€â”€ precompute.rs         #     Pre-computation utilities
â”‚   â”‚   â”‚   â”œâ”€â”€ projections.rs        #     Tensor projections
â”‚   â”‚   â”‚   â””â”€â”€ profiler/             #     Layer profiling
â”‚   â”‚   â”‚       â””â”€â”€ tensor_analysis.rs
â”‚   â”‚   â””â”€â”€ training/                 #   Training engine
â”‚   â”‚       â”œâ”€â”€ config.rs             #     TrainingConfig + SurgeryConfig
â”‚   â”‚       â”œâ”€â”€ datasets.rs           #     Dataset parsing (JSON, JSONL, CSV, Parquet)
â”‚   â”‚       â”œâ”€â”€ venv.rs               #     Python venv management
â”‚   â”‚       â”œâ”€â”€ scripts.rs            #     Training script generation
â”‚   â”‚       â”œâ”€â”€ executor.rs           #     Training subprocess management
â”‚   â”‚       â””â”€â”€ surgery.rs            #     Layer surgery (pure Rust)
â”‚   â”‚
â”‚   â”œâ”€â”€ Cargo.toml                    #   Rust dependencies
â”‚   â””â”€â”€ tauri.conf.json               #   Tauri configuration (1100Ã—720 window)
â”‚
â”œâ”€â”€ docs/                             # Documentation & assets
â”‚   â””â”€â”€ images/light/                 #   Screenshots
â”‚
â”œâ”€â”€ package.json                      # Node.js dependencies
â””â”€â”€ README.md                         # This file
```

---

## ğŸ“„ License

MIT â€” see [LICENSE](LICENSE) for details.

---

<p align="center">
  <strong>Built with ğŸ”¨ by the ForgeAI team</strong>
  <br/>
  <sub>Tauri Â· Svelte Â· Rust Â· Candle Â· llama.cpp</sub>
</p>
